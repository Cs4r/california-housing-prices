---
title: "Práctica 2: Limpieza y análisis de datos"
author: "César Aguilera"
date: "14/12/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Rcmdr)
library(nortest)
```

# 0. Carga del archivo

Se abre el archivo de datos y se examina el tipo de datos con los que R ha interpretado cada variable. Examinaremos también los valores resumen de cada tipo de variable.

```{r}
houses = read.csv(file = './data/housing.csv')
```

```{r}
head(houses)
```

## 0.1 Atributos / Nombres de columna
```{r}
names(houses)
```

## 0.2 Dimensiones
```{r}
dims = dim(houses)
dims
```
  
```{r}
print(paste("Filas: ", dims[1]))
print(paste("Columnas: ", dims[2]))
```

## 0.3 Tipo de datos con los que R ha interpretado cada variable

```{r}
sapply(houses,class)
```

## 0.4 Comprobar si hay valores perdidos

```{r}
any(is.na(houses))
```

## 0.5 Resumen de cada tipo de variable

```{r}
summary(houses)
```

# 1. Descripción del dataset

**¿Por qué es importante y qué pregunta/problema pretende responder?**

Este conjunto de datos se utiliza en el segundo capítulo del libro de Aurélien Géron *'Hands-On Machine learning with Scikit-Learn and TensorFlow'*. Sirve como una excelente introducción a la implementación de algoritmos de Machine Learning porque requiere una limpieza de datos preliminar, tiene una lista de variables fácilmente comprensible y tiene un tamaño óptimo: no es demasiado de juguete y ni demasiado difícil.

Los datos contienen información sobre el censo de California de 1990. Aunque puede que no nos ayuden a predecir los precios actuales de la vivienda como el conjunto de datos Zillow Zestimate (https://www.kaggle.com/c/zillow-prize-1), si que proporciona un conjunto de datos introductorio y accesible para aprender los conceptos básicos del aprendizaje automático.

El dataset contiene datos refrentes a casas pertenecientes a distrito determinado de California y algunas estadísticas resumidas sobre ellas basadas en los datos del censo de 1990. Debemos tener en cuenta que los datos están limpios, es decir, requieren limpieza previa.

El dataset tiene 20640 filas y 10 columnas. Las columnas son las siguientes:

* **longitude**: una medida de qué tan al oeste está una casa; un valor más alto está más al oeste
* **latitude**: medida de la distancia al norte de una casa; un valor más alto está más al norte
* **housingMedianAge**: edad promedio de una casa dentro de un bloque; un número menor es un edificio más nuevo
* **totalRooms**: número total de habitaciones dentro de un bloque
* **totalBedrooms**: número total de dormitorios dentro de un bloque
* **population**: número total de personas que residen dentro de un bloque
* **households**: número total de hogares, un grupo de personas que residen dentro de una unidad de vivienda, para un bloque
* **medianIncome**: ingresos medios para hogares dentro de un bloque de casas (medidos en decenas de miles de dólares estadounidenses)
* **medianHouseValue**: valor medio de la vivienda para los hogares dentro de un bloque (medido en dólares estadounidenses)
* **oceanProximity**: ubicación de la casa con respecto al océano / mar

Fuente: https://www.kaggle.com/camnugent/california-housing-prices

# 2. Integración y selección de los datos de interés a analizar

La integración o fusión de los datos consiste en la combinación de datos procedentes de múltiples fuentes, con el fin de crear una estructura de datos coherente y única que contenga mayor cantidad de información.

Esa fusión puede hacerse de dos formas:

1. De forma horizontal, añadiendo nuevos atributos a la base de datos original
2. De forma vertical, incluyendo nuevos registros a la base de datos original

**=> En este ejercicio NO vamos a incluir ningún tipo de integración de datos**

La selección de datos consiste en la elección de aquellos registros y variables de interés o relevantes para el problema a resolver.

En cuanto a la selección de datos para este ejercicio, nos vamos a quedar con todas las variables **salvo** longitude y latitude: ya que no nos parecen relevantes.

Eliminamos las variables longitude y latitude:

```{r}
houses$longitude <- NULL
houses$latitude <- NULL
```

Y seleccionamos (nos quedamos con) las siguientes columnas: 

* housing_median_age
* total_rooms
* total_bedrooms
* population
* households
* median_income
* median_house_value
* ocean_proximity


```{r}
names(houses)
```

```{r}
head(houses)
```

```{r}
summary(houses)
```

# 3. Limpieza de los datos

## 3.1.1 ¿Los datos contienen ceros o elementos vacíos?

En este apartado vamos a investigar las variables que contengan valores perdidos (NA).

```{r}
df = houses
 
for (colName in colnames(df)){
    hasNulls = any(is.na(df[colName]))
    if(hasNulls){
        print(paste(colName, "tiene valores perdidos"))
    }
}
```

La única variable que contiene valores perdidos (NA) es total_bedrooms.

```{r}
sum(is.na(houses$total_bedrooms))
```

total_bedrooms contiene 207 valores perdidos.

## 3.1.2 ¿Cómo gestionarías cada uno de estos casos?

Una vez investigadas las variables que contienen valores perdidos (NA) vamos a realizar una imputación de valores.

Para realizar la imputación de valores, voy a crear una función propia que realice una imputación de valores basada en la mediana del valor de la variable en el conjunto de datos.

**Programa para imputar valores**

Para obtener más precisión, voy a hacer la imputación solamente con datos de la misma ocean_proximity que la del registro que contiene el dato a imputar.

```{r}

nanIndexes = which(is.na(df$total_bedrooms))
medians = c()
pos = 1

# Compute medians and store them in a vector
for (index in nanIndexes){
    proximity = houses$ocean_proximity[index]

    # cut slice, with proximity
    slice = subset(houses, houses$ocean_proximity == proximity)$total_bedrooms
    
    # Compute the median of the slice
    sliceMedian = median(slice, na.rm = TRUE)

    # Store median
    medians[pos] = sliceMedian
    pos = pos + 1
}

pos = 1

# Set the values from medians vector
for(index in nanIndexes){
    houses$total_bedrooms[index] = medians[pos]
    pos = pos + 1
}

```
  
Muestra del resultado de la imputación en aquellos casos donde había un NA.

```{r}

i = 0

for (index in nanIndexes){
  print(sprintf("total_bedrooms[%d] set to %0.2f", index, houses$total_bedrooms[index]))
  i = i +1
  if(i == 21) break
}

```


## 3.2. Identificación y tratamiento de valores extremos

### 3.2.1 Identificación de valores extremos

En esta sección vamos a crear un programita que nos diga qué variables tienen valores extremos.


```{r}
df = houses

for (i in 1:ncol(df)){
    if(!is.numeric(df[,names(houses)[i]])){
      next
    }
  
   print(paste("La variable", names(houses)[i] , "tiene", length(boxplot.stats((df[,names(houses)[i]]))$out), "valores perdidos"))
}
```


Las variables con valores extremos son:

* total_rooms
* total_bedrooms
* population
* households
* median_income
* median_house_value


### 3.2.2 Tratamiento de valores extremos

En esta seccion vamos a sustituir valores atípicos por la mediana de la correspondiente variable.

**Variable total_rooms**

```{r}

outliers = boxplot.stats(houses$total_rooms)$out

for (out in outliers){
  houses$total_rooms[houses$total_rooms == out] <- median(houses$total_rooms)
}

```

**Variable total_bedrooms**

```{r}

outliers = boxplot.stats(houses$total_bedrooms)$out

for (out in outliers){
  houses$total_bedrooms[houses$total_bedrooms == out] <- median(houses$total_bedrooms)
}

```
  
**Variable population**

```{r}

outliers = boxplot.stats(houses$population)$out

for (out in outliers){
  houses$population[houses$population == out] <- median(houses$population)
}

```

**Variable households**

```{r}

outliers = boxplot.stats(houses$households)$out

for (out in outliers){
  houses$households[houses$households == out] <- median(houses$households)
}

```

**Variable median_income**

```{r}

outliers = boxplot.stats(houses$median_income)$out

for (out in outliers){
  houses$median_income[houses$median_income == out] <- median(houses$median_income)
}

```

**Variable median_house_value**

```{r}

outliers = boxplot.stats(houses$median_house_value)$out

for (out in outliers){
  houses$median_house_value[houses$median_house_value == out] <- median(houses$median_house_value)
}

```


# 4. Análisis de los datos

## 4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar)


## 4.2. Comprobación de la normalidad y homogeneidad de la varianza

### 4.2.0 Compración de la normalidad

**Hipótesis**

* H0: La muestra proviene de una distribución normal
* H1: La muestra no proviene de una distribución normal

Para pruebas de normalidad siempre se plantean así las hipótesis.

**Nivel de Significancia**

El nivel de significancia que se trabajará es de 0.05. Alfa=0.05

**Criterio de Decisión**

Si P < Alfa Se rechaza H0

Si p >= Alfa NO se rechaza H0

Donde P = p-valor

**Test a aplicar**

Vamos a aplicar el test de normalidad de Anderson-Darling, que funciona para variables con mas de 5000 muestras.

### 4.2.1 Comprobación de la normalidad de la variable housing_median_age

```{r}
hist(houses$housing_median_age)
```

A primera vista el histograma no nos dice mucho si la variable sigue una distribución normal o no.

Apliquemos ahora el test de normalidad.

```{r}
ad.test(houses$housing_median_age)
```

El p-valor es menor a Alpha (0.05), se rechaza la hipótesis nula. La variable housing_median_age NO sigue un distribución normal.

### 4.2.2 Comprobación de la normalidad para el resto de variables

```{r}
df = houses
alpha = 0.05
 
for (i in 2:ncol(df)){
    if(!is.numeric(df[,names(houses)[i]])){
      next
    }
  
    p = ad.test(df[,names(houses)[i]])$p.value
    
    if (p < alpha){
      print(paste(names(df)[i], "NO sigue una distribución normal"))
    }else if(p>=alpha){
      print(paste(names(df)[i], "SIGUE una distribución normal"))
    }
}
```

Como vemos, ninguna de las variables sigue una distribución normal acorde al test de normalidad de Anderson-Darling.

## 4.3 Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.


# 5. Representación de los resultados a partir de tablas y gráficas 


# 6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?
