---
title: "Práctica 2: Limpieza y análisis de datos"
author: "César Aguilera"
date: "14/12/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Rcmdr)
library(nortest)
```

# 0. Carga del archivo

Se abre el archivo de datos y se examina el tipo de datos con los que R ha interpretado cada variable. Examinaremos también los valores resumen de cada tipo de variable.

```{r}
houses = read.csv(file = './data/housing.csv')
```

```{r}
head(houses)
```

## 0.1 Atributos / Nombres de columna
```{r}
names(houses)
```

## 0.2 Dimensiones
```{r}
dims = dim(houses)
dims
```
  
```{r}
print(paste("Filas: ", dims[1]))
print(paste("Columnas: ", dims[2]))
```

## 0.3 Tipo de datos con los que R ha interpretado cada variable

```{r}
sapply(houses,class)
```

## 0.4 Comprobar si hay valores perdidos

```{r}
any(is.na(houses))
```

## 0.5 Resumen de cada tipo de variable

```{r}
summary(houses)
```

# 1. Descripción del dataset

**¿Por qué es importante y qué pregunta/problema pretende responder?**

Este conjunto de datos se utiliza en el segundo capítulo del libro de Aurélien Géron *'Hands-On Machine learning with Scikit-Learn and TensorFlow'*. Sirve como una excelente introducción a la implementación de algoritmos de Machine Learning porque requiere una limpieza de datos preliminar, tiene una lista de variables fácilmente comprensible y tiene un tamaño óptimo: no es demasiado de juguete y ni demasiado difícil.

Los datos contienen información sobre el censo de California de 1990. Aunque puede que no nos ayuden a predecir los precios actuales de la vivienda como el conjunto de datos Zillow Zestimate (https://www.kaggle.com/c/zillow-prize-1), si que proporciona un conjunto de datos introductorio y accesible para aprender los conceptos básicos del aprendizaje automático.

El dataset contiene datos refrentes a casas pertenecientes a distrito determinado de California y algunas estadísticas resumidas sobre ellas basadas en los datos del censo de 1990. Debemos tener en cuenta que los datos están limpios, es decir, requieren limpieza previa.

El dataset tiene 20640 filas y 10 columnas. Las columnas son las siguientes:

* **longitude**: una medida de qué tan al oeste está una casa; un valor más alto está más al oeste
* **latitude**: medida de la distancia al norte de una casa; un valor más alto está más al norte
* **housing_median_age**: edad promedio de una casa dentro de un bloque; un número menor es un edificio más nuevo
* **total_rooms**: número total de habitaciones dentro de un bloque
* **total_bedrooms**: número total de dormitorios dentro de un bloque
* **population**: número total de personas que residen dentro de un bloque
* **households**: número total de hogares, un grupo de personas que residen dentro de una unidad de vivienda, para un bloque
* **median_income**: ingresos medios para hogares dentro de un bloque de casas (medidos en decenas de miles de dólares estadounidenses)
* **median_house_value**: valor medio de la vivienda para los hogares dentro de un bloque (medido en dólares estadounidenses)
* **oceanProximity**: ubicación de la casa con respecto al océano / mar

Fuente: https://www.kaggle.com/camnugent/california-housing-prices

# 2. Integración y selección de los datos de interés a analizar

La integración o fusión de los datos consiste en la combinación de datos procedentes de múltiples fuentes, con el fin de crear una estructura de datos coherente y única que contenga mayor cantidad de información.

Esa fusión puede hacerse de dos formas:

1. De forma horizontal, añadiendo nuevos atributos a la base de datos original
2. De forma vertical, incluyendo nuevos registros a la base de datos original

**=> En este ejercicio NO vamos a incluir ningún tipo de integración de datos**

La selección de datos consiste en la elección de aquellos registros y variables de interés o relevantes para el problema a resolver.

En cuanto a la selección de datos para este ejercicio, nos vamos a quedar con todas las variables **salvo** longitude y latitude: ya que no nos parecen relevantes.

Eliminamos las variables longitude y latitude:

```{r}
houses$longitude <- NULL
houses$latitude <- NULL
```

Y seleccionamos (nos quedamos con) las siguientes columnas: 

* housing_median_age
* total_rooms
* total_bedrooms
* population
* households
* median_income
* median_house_value
* ocean_proximity


```{r}
names(houses)
```

```{r}
head(houses)
```

```{r}
summary(houses)
```

# 3. Limpieza de los datos

## 3.1.1 ¿Los datos contienen ceros o elementos vacíos?

En este apartado vamos a investigar las variables que contengan valores perdidos (NA).

```{r}
df = houses
 
for (colName in colnames(df)){
    hasNulls = any(is.na(df[colName]))
    if(hasNulls){
        print(paste(colName, "tiene valores perdidos"))
    }
}
```

La única variable que contiene valores perdidos (NA) es total_bedrooms.

```{r}
sum(is.na(houses$total_bedrooms))
```

total_bedrooms contiene 207 valores perdidos.

## 3.1.2 ¿Cómo gestionarías cada uno de estos casos?

Una vez investigadas las variables que contienen valores perdidos (NA) vamos a realizar una imputación de valores.

Para realizar la imputación de valores, voy a crear una función propia que realice una imputación de valores basada en la mediana del valor de la variable en el conjunto de datos.

**Programa para imputar valores**

Para obtener más precisión, voy a hacer la imputación solamente con datos de la misma ocean_proximity que la del registro que contiene el dato a imputar.

```{r}

nanIndexes = which(is.na(df$total_bedrooms))
medians = c()
pos = 1

# Compute medians and store them in a vector
for (index in nanIndexes){
    proximity = houses$ocean_proximity[index]

    # cut slice, with proximity
    slice = subset(houses, houses$ocean_proximity == proximity)$total_bedrooms
    
    # Compute the median of the slice
    sliceMedian = median(slice, na.rm = TRUE)

    # Store median
    medians[pos] = sliceMedian
    pos = pos + 1
}

pos = 1

# Set the values from medians vector
for(index in nanIndexes){
    houses$total_bedrooms[index] = medians[pos]
    pos = pos + 1
}

```
  
Muestra del resultado de la imputación en aquellos casos donde había un NA.

```{r}

i = 0

for (index in nanIndexes){
  print(sprintf("total_bedrooms[%d] set to %0.2f", index, houses$total_bedrooms[index]))
  i = i +1
  if(i == 21) break
}

```


## 3.2. Identificación y tratamiento de valores extremos

### 3.2.1 Identificación de valores extremos

En esta sección vamos a crear un programita que nos diga qué variables tienen valores extremos.


```{r}
df = houses

for (i in 1:ncol(df)){
    if(!is.numeric(df[,names(houses)[i]])){
      next
    }
  
   print(paste("La variable", names(houses)[i] , "tiene", length(boxplot.stats((df[,names(houses)[i]]))$out), "valores perdidos"))
}
```


Las variables con valores extremos son:

* total_rooms
* total_bedrooms
* population
* households
* median_income
* median_house_value


### 3.2.2 Tratamiento de valores extremos

En esta seccion vamos a sustituir valores atípicos por la mediana de la correspondiente variable.

**Variable total_rooms**

```{r}

outliers = boxplot.stats(houses$total_rooms)$out

for (out in outliers){
  houses$total_rooms[houses$total_rooms == out] <- median(houses$total_rooms)
}

```

**Variable total_bedrooms**

```{r}

outliers = boxplot.stats(houses$total_bedrooms)$out

for (out in outliers){
  houses$total_bedrooms[houses$total_bedrooms == out] <- median(houses$total_bedrooms)
}

```
  
**Variable population**

```{r}

outliers = boxplot.stats(houses$population)$out

for (out in outliers){
  houses$population[houses$population == out] <- median(houses$population)
}

```

**Variable households**

```{r}

outliers = boxplot.stats(houses$households)$out

for (out in outliers){
  houses$households[houses$households == out] <- median(houses$households)
}

```

**Variable median_income**

```{r}

outliers = boxplot.stats(houses$median_income)$out

for (out in outliers){
  houses$median_income[houses$median_income == out] <- median(houses$median_income)
}

```

**Variable median_house_value**

```{r}

outliers = boxplot.stats(houses$median_house_value)$out

for (out in outliers){
  houses$median_house_value[houses$median_house_value == out] <- median(houses$median_house_value)
}

```


# 4. Análisis de los datos

## 4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar)

En esta sección hemos elegido distintos grupos de datos.

Primeramente, escogemos todas las variables para comprobar cuales siguen una distribución normal y cuales no.

Seguidamente seleccionamos la variable población (population) para estudiar la homogeneidad de su varianza dependiendo de si la población está cerca de la bahía o cerca del océano.

Finalmente elegimos las variables median_income y median_house_value para estudiar si existe o no correlación entre ambas.

## 4.2. Comprobación de la normalidad y homogeneidad de la varianza

### 4.2.0 Compración de la normalidad

**Hipótesis**

* H0: La muestra proviene de una distribución normal
* H1: La muestra no proviene de una distribución normal

Para pruebas de normalidad siempre se plantean así las hipótesis.

**Nivel de Significancia**

El nivel de significancia que se trabajará es de 0.05. Alfa=0.05

**Criterio de Decisión**

Si P < Alfa Se rechaza H0

Si p >= Alfa NO se rechaza H0

Donde P = p-valor

**Test a aplicar**

Vamos a aplicar el test de normalidad de Anderson-Darling, que funciona para variables con mas de 5000 muestras.

### 4.2.1 Comprobación de la normalidad de la variable housing_median_age

```{r}
hist(houses$housing_median_age)
```

A primera vista el histograma no nos dice mucho si la variable sigue una distribución normal o no.

Apliquemos ahora el test de normalidad.

```{r}
ad.test(houses$housing_median_age)
```

El p-valor es menor a Alpha (0.05), se rechaza la hipótesis nula. La variable housing_median_age NO sigue un distribución normal.

### 4.2.2 Comprobación de la normalidad para el resto de variables

En esta ocasión hemos creado un programa que comprueba la normalidad de todas las variables del conjunto de datos.

```{r}
df = houses
alpha = 0.05
 
for (i in 2:ncol(df)){
    if(!is.numeric(df[,names(houses)[i]])){
      next
    }
  
    p = ad.test(df[,names(houses)[i]])$p.value
    
    if (p < alpha){
      print(paste(names(df)[i], "NO sigue una distribución normal"))
    }else if(p>=alpha){
      print(paste(names(df)[i], "SIGUE una distribución normal"))
    }
}
```

Como vemos, el test de normalidad de Anderson-Darling da negativo para todas las variables, es decir, ninguna sigue una distribución normal.

### 4.2.3 Compración de la homogeneidad de la varianza

**Hipótesis**

* H0: La varianza es igual entre los grupos
* H1: La varianza NO es igual entre los grupos

**Nivel de Significancia**

El nivel de significancia que se trabajará es de 0.05. Alfa=0.05

**Criterio de Decisión**

Si P < Alfa Se rechaza H0

Si p >= Alfa NO se rechaza H0

Donde P = p-valor

**Test a aplicar**

Vamos a aplicar el test de Fligner-Killeen puesto que es uno de los más adecuados cuando no se cumple la condición de normalidad en las muestras.

### 4.2.4 Compración de la homogeneidad de la varianza de población entre casas cerca de la bahía y cerca del océano

```{r}
a <- houses[houses$ocean_proximity =="NEAR BAY", "population"]
b <- houses[houses$ocean_proximity =="NEAR OCEAN", "population"]
fligner.test(x = list(a,b))
```

El p-valor (0.006041) es menor a Alpha (0.05), se rechaza la hipótesis nula. El test encuentra diferencias significativas entre las varianzas de los dos grupos.

## 4.3 Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

Primeramente vamos a empezar calculando el intervalo de confianza para la media de la variable median_house_value. El intervalo de confianza permite calcular dos valores alrededor de una media muestral (uno superior y otro inferior). Estos dos valores van a acotar un rango dentro del cual, con una determinada probabilidad, se va a localizar el parámetro de la media poblacional.

**El intervalo de confianza calculado será por defecto del 95%.

```{r}
t.test(houses$median_house_value)
```

El intervalo de confianza para la variables median_house_value nos indica que la probabilidad de que la media poblacional *μ* pertenezca a un intervalo de la forma: [186393.3, 189033.3] es de 0.95. O lo que es lo mismo: noventa y cinco de cada cien veces que escogemos una muestra aleatoria simple y calculamos el valor de la media muestral, el intervalo que obtendremos sustituyendo el valor de X̅̄ por la media correspondiente a la muestra de la que disponemos contendrá el verdadero valor de *μ*.

Seguidamente vamos a utilizar el contraste de hipótesis para evaluar si el valor medio de la vivienda para los hogares dentro de un bloque (median_house_value) es superior en los bloques cerca de la bahía con respecto a los que NO están cerca de la bahía.


```{r}
nearBay <- houses[houses$ocean_proximity == "NEAR BAY", "median_house_value"]
farBay <- houses[houses$ocean_proximity != "NEAR BAY", "median_house_value"]
```

**Hipótesis nula**

La hipótesis nula (H0) afirma que los valores de las medias de las dos poblaciones son iguales. Es decir, la media poblacional del valor medio de la vivienda para los hogares dentro de un bloque es igual en los bloques cerca de la bahía que los que NO están cerca de la bahía: *μ1* = *μ2*.

Otra manera de ver la hipótesis nula es *μ1* - *μ2* = 0.

**Hipótesis alternativa**

La hipotesis alternativa (H1) afirma que la media de la población 1 es superior a la media de la población 2. Es decir, que la media el del valor medio de la vivienda para los hogares dentro de un bloque (median_house_value) es superior en los bloques cerca de la bahía con respecto a los que NO están cerca de la bahía.: *μ1* > *μ2*

Otra manera de ver la hipótesis alternativa es *μ1* - *μ2* > 0.

**Test a aplicar**

Dado que no podemos asegurar que la variable median_house_value siga una distribuación normal, sólo podremos contrastar la diferencia de medias si los tamaños de las muestras son superiores a treinta => que en este caso se cumple.

En resumen, el test a aplicar es el contraste sobre la diferencia de medias en el caso de tener muestras grandes no normales.

**Cálculos**

Con un nivel de significación del 5%, ¿podemos asegurar que el valor medio de la vivienda es el mismo?

```{r}
x1 = mean(nearBay)
s1 = sd(nearBay)
n1 = length(nearBay)

x2 = mean(farBay)
s2 = sd(farBay)
n2 = length(farBay)

alpha = 0.05

# estadístico de contraste
z = (x1-x2)/sqrt((s1*s1)/n1 + (s2*s2)/n2)

# p-valor
p = 1 - pnorm(z)

print(paste("p-value is", p))

if(p < alpha){ 
  print("p less than alpha")
} else if (p == alpha){
  print("p equal to alpha")
} else {print("p greater than alpha")}
```

El p-valor obtenido es cero. Y en concreto menor que el nivel de significación. 

Diremos que el p-valor es significativo y rechazamos la hipótesis nula en favor de la hipótesis alternativa. Por ende se puede afirmar que el valor medio de la vivienda para los hogares dentro de un bloque (median_house_value) es superior en los bloques cerca de la bahía con respecto a los que NO están cerca de la bahía, y esto se afirma con un 95% de nivel de confianza.

Finalmente, vamos a hallar la correlación entre los ingresos medios para hogares dentro de un bloque de casas (median_income)
y el valor medio de la vivienda para los hogares dentro de un bloque (median_house_value). Para ello vamos a calcular el coeficiente de correlación de Pearson, que es independiente de la escala de medida de las variables.

El coeficiente de correlación de Pearson es como un índice que puede utilizarse para medir el grado de relación de dos variables siempre y cuando ambas sean cuantitativas y continuas.

```{r}
cor.test(houses$median_income, houses$median_house_value, method=c("pearson"))
```

En este caso el coeficiente de correlación de Pearson (r) está entre 0 y 1, en concreto 0.58. Podemos decir que existe una correlación positiva entre median_income y median_house_value, aunque dicha correlación es débil.


# 5. Representación de los resultados a partir de tablas y gráficas 

## 5.1 Descriptiva y visualización

A continuación vamos a realizar una visualización gráfica de los datos del conjunto de datos. También explicaremos brevemente los gráficos y lo que se puede observar a partir de ellos.

### 5.1.1 Histogramas

```{r}
par(mfrow=c(1,3))

hist(houses$housing_median_age,
main="Histograma de housing_median_age",     
xlab="edad promedio de una casa dentro de un bloque",
ylab="Frecuencia",
col="cornflowerblue",
)


hist(houses$total_rooms,
main="Histograma de total_rooms",     
xlab="número de habitaciones dentro de un bloque",
ylab="Frecuencia",
col="cornflowerblue",
)

hist(houses$total_bedrooms,
main="Histograma de total_bedrooms",     
xlab="número de dormitorios dentro de un bloque",
ylab="Frecuencia",
col="cornflowerblue",
)

###### 

hist(houses$population,
main="Histograma de population",       
xlab="número de personas que residen dentro de un bloque",
ylab="Frecuencia",
col="firebrick1",
)

hist(houses$households,
main="Histograma de households",       
xlab="número total de hogares",
ylab="Frecuencia",
col="firebrick1",
)

hist(houses$median_income,
main="Histograma de median_income",       
xlab="ingresos medios para hogares dentro de un bloque de casas",
ylab="Frecuencia",
col="firebrick1",
)

####

hist(houses$median_house_value,
main="Histograma de median_house_value",       
xlab="valor medio de la vivienda para los hogares dentro de un bloque",
ylab="Frecuencia",
col="darkseagreen1",
)

```

### 5.1.2 Gráficos de barras

```{r}
par(mfrow=c(1,1))

Barplot(houses$ocean_proximity, main= "Gráfico de barras para ocean_proximity", xlab="ubicación de la casa con respecto al océano / mar", ylab="Frecuencia", col="cornflowerblue")
```

### 5.1.3 Diagramas de caja

```{r}
par(mfrow=c(1,3))

boxplot(houses$housing_median_age, ylab = "housing_median_age", col = "cornflowerblue")
boxplot(houses$total_rooms, ylab = "total_rooms", col = "cornflowerblue")
boxplot(houses$total_bedrooms, ylab = "total_bedrooms", col = "cornflowerblue")
```

En el gráfico de **housing_median_age** se observa una mediana aproximada de 29, un mínimo de 1, un máximo de 52 y ningún valor atípico.

En el gráfico de **total_rooms** se observa una mediana aproximada de 2127, un mínimo de 2, un máximo de 5694 y numerosos valores atípicos entre 4883 y 5694.

En el gráfico de **total_bedrooms** se observa una mediana de 435, un mínimo de 1, un máximo de 1163 y numerosos valores atípicos entre 993 y 1163.

```{r}
par(mfrow=c(1,3))
boxplot(houses$population, ylab = "population", col = "firebrick1")
boxplot(houses$households, ylab = "households", col = "firebrick1")
boxplot(houses$median_income, ylab = "median_income", col = "firebrick1")
```

En el gráfico de **population** se observa una mediana de 1166, un mínimo de 3, un máximo de 3132 y numerosos valores atípicos por encima del máximo.

El gráfico de **households** presenta numerosos valores atípicos en el rango [941, 1092]. La mediana es 409, el mínimo 1 y el máximo 1092.

En el gráfico de **median_income** se observa una mediana aproximada de 3.53, un mínimo de 0.5, un máximo de 8.01 y 247 valores atípicos en el rango [7.52, 8.0137].

```{r}
par(mfrow=c(1,3))
boxplot(houses$median_house_value, ylab = "median_house_value", col = "darkseagreen1")
```

En el gráfico de **median_house_value** se observa una mediana aproximada de 179700, un mínimo de 14999, un máximo de 482200 y 435 valores atípicos en el rango [424400, 482200].


### 5.1.2 Gráficos cirulares

```{r}

customPie <- function(slices, theTitle) {
  lbls = levels(slices)
  slices = table(slices)
  pct <- round(slices/sum(slices)*100)
  lbls <- paste(lbls, pct) # add percents to labels
  lbls <- paste(lbls,"%",sep="") # ad % to labels
  pie(slices,labels = lbls, col=rainbow(length(lbls)),main=theTitle)
}
```

```{r}

customPie(houses$ocean_proximity, "ubicación de la casa con respecto al océano / mar")

```

En el gráfico de **ocean_proximity** se aprecia que el valor que más aparece es *Ocean*, con el 44% de la veces, seguido del valor *Inland* que aparece el 32% de las veces, el valor *Near ocean* con un 13%, el valor **Near bay* con un 11% y finalmente el valor *Island* que aparece el 0.00024%. 

# 6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?



